{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6393f9fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] NLTK resource 'punkt' (tokenizers/punkt) sudah ada.\n",
      "[INFO] NLTK resource 'stopwords' (corpora/stopwords) sudah ada.\n",
      "[INFO] NLTK resource 'averaged_perceptron_tagger' (taggers/averaged_perceptron_tagger) sudah ada.\n",
      "[INFO] NLTK resource 'wordnet' (corpora/wordnet) tidak ditemukan. Mengunduh...\n",
      "[INFO] Berhasil mengunduh 'wordnet'.\n",
      "[INFO] NLTK resource 'omw-1.4' (corpora/omw-1.4) tidak ditemukan. Mengunduh...\n",
      "[INFO] Berhasil mengunduh 'omw-1.4'.\n",
      "[INFO] NLTK resource 'sentiwordnet' (corpora/sentiwordnet) sudah ada.\n",
      "--------------------------------------------------\n",
      "[INFO] Berhasil memuat file: Data Pakai Label.csv\n",
      "Jumlah baris awal: 10798\n",
      "Kolom yang ada: ['userName', 'content', 'score', 'reviewCreatedVersion', 'at', 'Sentimen (Aurel)', 'Sentimen (Ade)', 'Sentimen (Eky)', 'Sentimen Akhir']\n",
      "        userName                                            content  score  \\\n",
      "0  Made Darmawan  Saya kasi bintang 1 dulu ya. Tolong diperbaiki...      1   \n",
      "1  windi tripani  Apknya memudahkan dalam perubahan data, ngecek...      5   \n",
      "\n",
      "  reviewCreatedVersion                   at  Sentimen (Aurel)  Sentimen (Ade)  \\\n",
      "0                4.3.0  2023-01-22 12:04:08              -1.0            -1.0   \n",
      "1                4.3.0  2023-01-27 06:56:51               1.0             1.0   \n",
      "\n",
      "   Sentimen (Eky) Sentimen Akhir  \n",
      "0            -1.0        Negatif  \n",
      "1             1.0        Positif  \n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/xerces/anaconda3/envs/venv/lib/python3.13/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Memulai proses translasi...\n",
      "[INFO] Memuat model translasi: Helsinki-NLP/opus-mt-id-en\n",
      "[INFO] Model translasi dimuat.\n",
      "[INFO] Menerjemahkan batch 1/1350 (ukuran: 8)...\n",
      "[INFO] Menerjemahkan batch 2/1350 (ukuran: 8)...\n",
      "[INFO] Menerjemahkan batch 3/1350 (ukuran: 8)...\n",
      "[INFO] Menerjemahkan batch 4/1350 (ukuran: 8)...\n",
      "[INFO] Menerjemahkan batch 5/1350 (ukuran: 8)...\n",
      "[INFO] Menerjemahkan batch 6/1350 (ukuran: 8)...\n",
      "[INFO] Menerjemahkan batch 7/1350 (ukuran: 8)...\n",
      "[INFO] Menerjemahkan batch 8/1350 (ukuran: 8)...\n",
      "[INFO] Menerjemahkan batch 9/1350 (ukuran: 8)...\n",
      "[INFO] Menerjemahkan batch 10/1350 (ukuran: 8)...\n",
      "[INFO] Menerjemahkan batch 11/1350 (ukuran: 8)...\n",
      "[INFO] Menerjemahkan batch 12/1350 (ukuran: 8)...\n",
      "[INFO] Menerjemahkan batch 13/1350 (ukuran: 8)...\n",
      "[INFO] Menerjemahkan batch 14/1350 (ukuran: 8)...\n",
      "[INFO] Menerjemahkan batch 15/1350 (ukuran: 8)...\n",
      "[INFO] Menerjemahkan batch 16/1350 (ukuran: 8)...\n",
      "[INFO] Menerjemahkan batch 17/1350 (ukuran: 8)...\n",
      "[INFO] Menerjemahkan batch 18/1350 (ukuran: 8)...\n",
      "[INFO] Menerjemahkan batch 19/1350 (ukuran: 8)...\n",
      "[INFO] Menerjemahkan batch 20/1350 (ukuran: 8)...\n",
      "[INFO] Menerjemahkan batch 21/1350 (ukuran: 8)...\n",
      "[INFO] Menerjemahkan batch 22/1350 (ukuran: 8)...\n",
      "[INFO] Menerjemahkan batch 23/1350 (ukuran: 8)...\n",
      "[INFO] Menerjemahkan batch 24/1350 (ukuran: 8)...\n",
      "[INFO] Menerjemahkan batch 25/1350 (ukuran: 8)...\n",
      "[INFO] Menerjemahkan batch 26/1350 (ukuran: 8)...\n",
      "[INFO] Menerjemahkan batch 27/1350 (ukuran: 8)...\n",
      "[INFO] Menerjemahkan batch 28/1350 (ukuran: 8)...\n",
      "[INFO] Menerjemahkan batch 29/1350 (ukuran: 8)...\n",
      "[INFO] Menerjemahkan batch 30/1350 (ukuran: 8)...\n",
      "[INFO] Menerjemahkan batch 31/1350 (ukuran: 8)...\n",
      "[INFO] Menerjemahkan batch 32/1350 (ukuran: 8)...\n",
      "[INFO] Menerjemahkan batch 33/1350 (ukuran: 8)...\n",
      "[INFO] Menerjemahkan batch 34/1350 (ukuran: 8)...\n",
      "[INFO] Menerjemahkan batch 35/1350 (ukuran: 8)...\n",
      "[INFO] Menerjemahkan batch 36/1350 (ukuran: 8)...\n",
      "[INFO] Menerjemahkan batch 37/1350 (ukuran: 8)...\n",
      "[INFO] Menerjemahkan batch 38/1350 (ukuran: 8)...\n",
      "[INFO] Menerjemahkan batch 39/1350 (ukuran: 8)...\n",
      "[INFO] Menerjemahkan batch 40/1350 (ukuran: 8)...\n",
      "[INFO] Menerjemahkan batch 41/1350 (ukuran: 8)...\n",
      "[INFO] Menerjemahkan batch 42/1350 (ukuran: 8)...\n",
      "[INFO] Menerjemahkan batch 43/1350 (ukuran: 8)...\n",
      "[INFO] Menerjemahkan batch 44/1350 (ukuran: 8)...\n",
      "[INFO] Menerjemahkan batch 45/1350 (ukuran: 8)...\n",
      "[INFO] Menerjemahkan batch 46/1350 (ukuran: 8)...\n",
      "[INFO] Menerjemahkan batch 47/1350 (ukuran: 8)...\n",
      "[INFO] Menerjemahkan batch 48/1350 (ukuran: 8)...\n",
      "[INFO] Menerjemahkan batch 49/1350 (ukuran: 8)...\n",
      "[INFO] Menerjemahkan batch 50/1350 (ukuran: 8)...\n",
      "[INFO] Menerjemahkan batch 51/1350 (ukuran: 8)...\n",
      "[INFO] Menerjemahkan batch 52/1350 (ukuran: 8)...\n",
      "[INFO] Menerjemahkan batch 53/1350 (ukuran: 8)...\n",
      "[INFO] Menerjemahkan batch 54/1350 (ukuran: 8)...\n",
      "[INFO] Menerjemahkan batch 55/1350 (ukuran: 8)...\n",
      "[INFO] Menerjemahkan batch 56/1350 (ukuran: 8)...\n",
      "[INFO] Menerjemahkan batch 57/1350 (ukuran: 8)...\n",
      "[INFO] Menerjemahkan batch 58/1350 (ukuran: 8)...\n",
      "[INFO] Menerjemahkan batch 59/1350 (ukuran: 8)...\n",
      "[INFO] Menerjemahkan batch 60/1350 (ukuran: 8)...\n",
      "[INFO] Menerjemahkan batch 61/1350 (ukuran: 8)...\n",
      "[INFO] Menerjemahkan batch 62/1350 (ukuran: 8)...\n",
      "[INFO] Menerjemahkan batch 63/1350 (ukuran: 8)...\n",
      "[INFO] Menerjemahkan batch 64/1350 (ukuran: 8)...\n",
      "[INFO] Menerjemahkan batch 65/1350 (ukuran: 8)...\n",
      "[INFO] Menerjemahkan batch 66/1350 (ukuran: 8)...\n",
      "[INFO] Menerjemahkan batch 67/1350 (ukuran: 8)...\n",
      "[INFO] Menerjemahkan batch 68/1350 (ukuran: 8)...\n",
      "[INFO] Menerjemahkan batch 69/1350 (ukuran: 8)...\n",
      "[INFO] Menerjemahkan batch 70/1350 (ukuran: 8)...\n",
      "[INFO] Menerjemahkan batch 71/1350 (ukuran: 8)...\n",
      "[INFO] Menerjemahkan batch 72/1350 (ukuran: 8)...\n",
      "[INFO] Menerjemahkan batch 73/1350 (ukuran: 8)...\n",
      "[INFO] Menerjemahkan batch 74/1350 (ukuran: 8)...\n",
      "[INFO] Menerjemahkan batch 75/1350 (ukuran: 8)...\n",
      "[INFO] Menerjemahkan batch 76/1350 (ukuran: 8)...\n",
      "[INFO] Menerjemahkan batch 77/1350 (ukuran: 8)...\n",
      "[INFO] Menerjemahkan batch 78/1350 (ukuran: 8)...\n",
      "[INFO] Menerjemahkan batch 79/1350 (ukuran: 8)...\n",
      "[INFO] Menerjemahkan batch 80/1350 (ukuran: 8)...\n",
      "[INFO] Menerjemahkan batch 81/1350 (ukuran: 8)...\n",
      "[INFO] Menerjemahkan batch 82/1350 (ukuran: 8)...\n",
      "[INFO] Menerjemahkan batch 83/1350 (ukuran: 8)...\n",
      "[INFO] Menerjemahkan batch 84/1350 (ukuran: 8)...\n",
      "[INFO] Menerjemahkan batch 85/1350 (ukuran: 8)...\n",
      "[INFO] Menerjemahkan batch 86/1350 (ukuran: 8)...\n",
      "[INFO] Menerjemahkan batch 87/1350 (ukuran: 8)...\n",
      "[INFO] Menerjemahkan batch 88/1350 (ukuran: 8)...\n",
      "[INFO] Menerjemahkan batch 89/1350 (ukuran: 8)...\n",
      "[INFO] Menerjemahkan batch 90/1350 (ukuran: 8)...\n",
      "[INFO] Menerjemahkan batch 91/1350 (ukuran: 8)...\n",
      "[INFO] Menerjemahkan batch 92/1350 (ukuran: 8)...\n",
      "[INFO] Menerjemahkan batch 93/1350 (ukuran: 8)...\n",
      "[INFO] Menerjemahkan batch 94/1350 (ukuran: 8)...\n",
      "[INFO] Menerjemahkan batch 95/1350 (ukuran: 8)...\n",
      "[INFO] Menerjemahkan batch 96/1350 (ukuran: 8)...\n",
      "[INFO] Menerjemahkan batch 97/1350 (ukuran: 8)...\n",
      "[INFO] Menerjemahkan batch 98/1350 (ukuran: 8)...\n",
      "[INFO] Menerjemahkan batch 99/1350 (ukuran: 8)...\n",
      "[INFO] Menerjemahkan batch 100/1350 (ukuran: 8)...\n",
      "[INFO] Menerjemahkan batch 101/1350 (ukuran: 8)...\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[12]\u001b[39m\u001b[32m, line 123\u001b[39m\n\u001b[32m    121\u001b[39m \u001b[38;5;66;03m# Ambil hanya kolom 'content' untuk diterjemahkan\u001b[39;00m\n\u001b[32m    122\u001b[39m content_to_translate = df_original[\u001b[33m'\u001b[39m\u001b[33mcontent\u001b[39m\u001b[33m'\u001b[39m].copy()\n\u001b[32m--> \u001b[39m\u001b[32m123\u001b[39m df_original[translated_column_name] = \u001b[43mtranslate_batch_hf\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcontent_to_translate\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    124\u001b[39m end_translate_time = time.time()\n\u001b[32m    125\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m[INFO] Proses translasi selesai dalam \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mend_translate_time\u001b[38;5;250m \u001b[39m-\u001b[38;5;250m \u001b[39mstart_translate_time\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m detik.\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[12]\u001b[39m\u001b[32m, line 107\u001b[39m, in \u001b[36mtranslate_batch_hf\u001b[39m\u001b[34m(texts, model_name, batch_size)\u001b[39m\n\u001b[32m    105\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    106\u001b[39m     inputs = tokenizer(batch_texts_cleaned, return_tensors=\u001b[33m\"\u001b[39m\u001b[33mpt\u001b[39m\u001b[33m\"\u001b[39m, padding=\u001b[38;5;28;01mTrue\u001b[39;00m, truncation=\u001b[38;5;28;01mTrue\u001b[39;00m, max_length=\u001b[32m512\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m107\u001b[39m     translated_tokens = \u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    108\u001b[39m     batch_translated = [tokenizer.decode(t, skip_special_tokens=\u001b[38;5;28;01mTrue\u001b[39;00m) \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m translated_tokens]\n\u001b[32m    109\u001b[39m     translated_texts.extend(batch_translated)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/venv/lib/python3.13/site-packages/torch/utils/_contextlib.py:116\u001b[39m, in \u001b[36mcontext_decorator.<locals>.decorate_context\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    113\u001b[39m \u001b[38;5;129m@functools\u001b[39m.wraps(func)\n\u001b[32m    114\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdecorate_context\u001b[39m(*args, **kwargs):\n\u001b[32m    115\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[32m--> \u001b[39m\u001b[32m116\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/venv/lib/python3.13/site-packages/transformers/generation/utils.py:2616\u001b[39m, in \u001b[36mGenerationMixin.generate\u001b[39m\u001b[34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, use_model_defaults, custom_generate, **kwargs)\u001b[39m\n\u001b[32m   2609\u001b[39m     input_ids, model_kwargs = \u001b[38;5;28mself\u001b[39m._expand_inputs_for_generation(\n\u001b[32m   2610\u001b[39m         input_ids=input_ids,\n\u001b[32m   2611\u001b[39m         expand_size=generation_config.num_beams,\n\u001b[32m   2612\u001b[39m         is_encoder_decoder=\u001b[38;5;28mself\u001b[39m.config.is_encoder_decoder,\n\u001b[32m   2613\u001b[39m         **model_kwargs,\n\u001b[32m   2614\u001b[39m     )\n\u001b[32m   2615\u001b[39m     \u001b[38;5;66;03m# 12. run beam sample\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m2616\u001b[39m     result = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_beam_search\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2617\u001b[39m \u001b[43m        \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2618\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlogits_processor\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprepared_logits_processor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2619\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstopping_criteria\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprepared_stopping_criteria\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2620\u001b[39m \u001b[43m        \u001b[49m\u001b[43mgeneration_config\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgeneration_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2621\u001b[39m \u001b[43m        \u001b[49m\u001b[43msynced_gpus\u001b[49m\u001b[43m=\u001b[49m\u001b[43msynced_gpus\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2622\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2623\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2625\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m generation_mode == GenerationMode.GROUP_BEAM_SEARCH:\n\u001b[32m   2626\u001b[39m     \u001b[38;5;66;03m# 11. prepare beam search scorer\u001b[39;00m\n\u001b[32m   2627\u001b[39m     beam_scorer = BeamSearchScorer(\n\u001b[32m   2628\u001b[39m         batch_size=batch_size,\n\u001b[32m   2629\u001b[39m         num_beams=generation_config.num_beams,\n\u001b[32m   (...)\u001b[39m\u001b[32m   2635\u001b[39m         max_length=generation_config.max_length,\n\u001b[32m   2636\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/venv/lib/python3.13/site-packages/transformers/generation/utils.py:4030\u001b[39m, in \u001b[36mGenerationMixin._beam_search\u001b[39m\u001b[34m(self, input_ids, logits_processor, stopping_criteria, generation_config, synced_gpus, **model_kwargs)\u001b[39m\n\u001b[32m   4027\u001b[39m model_inputs.update({\u001b[33m\"\u001b[39m\u001b[33moutput_attentions\u001b[39m\u001b[33m\"\u001b[39m: output_attentions} \u001b[38;5;28;01mif\u001b[39;00m output_attentions \u001b[38;5;28;01melse\u001b[39;00m {})\n\u001b[32m   4028\u001b[39m model_inputs.update({\u001b[33m\"\u001b[39m\u001b[33moutput_hidden_states\u001b[39m\u001b[33m\"\u001b[39m: output_hidden_states} \u001b[38;5;28;01mif\u001b[39;00m output_hidden_states \u001b[38;5;28;01melse\u001b[39;00m {})\n\u001b[32m-> \u001b[39m\u001b[32m4030\u001b[39m model_outputs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mmodel_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m   4032\u001b[39m \u001b[38;5;66;03m# synced_gpus: don't waste resources running the code we don't need; kwargs must be updated before skipping\u001b[39;00m\n\u001b[32m   4033\u001b[39m model_kwargs = \u001b[38;5;28mself\u001b[39m._update_model_kwargs_for_generation(\n\u001b[32m   4034\u001b[39m     model_outputs,\n\u001b[32m   4035\u001b[39m     model_kwargs,\n\u001b[32m   4036\u001b[39m     is_encoder_decoder=\u001b[38;5;28mself\u001b[39m.config.is_encoder_decoder,\n\u001b[32m   4037\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/venv/lib/python3.13/site-packages/torch/nn/modules/module.py:1739\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1737\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1738\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1739\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/venv/lib/python3.13/site-packages/torch/nn/modules/module.py:1750\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1745\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1746\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1747\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1748\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1749\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1750\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1752\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1753\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/venv/lib/python3.13/site-packages/transformers/models/marian/modeling_marian.py:1499\u001b[39m, in \u001b[36mMarianMTModel.forward\u001b[39m\u001b[34m(self, input_ids, attention_mask, decoder_input_ids, decoder_attention_mask, head_mask, decoder_head_mask, cross_attn_head_mask, encoder_outputs, past_key_values, inputs_embeds, decoder_inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict, cache_position)\u001b[39m\n\u001b[32m   1494\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m decoder_input_ids \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m decoder_inputs_embeds \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   1495\u001b[39m         decoder_input_ids = shift_tokens_right(\n\u001b[32m   1496\u001b[39m             labels, \u001b[38;5;28mself\u001b[39m.config.pad_token_id, \u001b[38;5;28mself\u001b[39m.config.decoder_start_token_id\n\u001b[32m   1497\u001b[39m         )\n\u001b[32m-> \u001b[39m\u001b[32m1499\u001b[39m outputs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1500\u001b[39m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1501\u001b[39m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1502\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdecoder_input_ids\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdecoder_input_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1503\u001b[39m \u001b[43m    \u001b[49m\u001b[43mencoder_outputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mencoder_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1504\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdecoder_attention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdecoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1505\u001b[39m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1506\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdecoder_head_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdecoder_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1507\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcross_attn_head_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcross_attn_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1508\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1509\u001b[39m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[43m=\u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1510\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdecoder_inputs_embeds\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdecoder_inputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1511\u001b[39m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[43m=\u001b[49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1512\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1513\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1514\u001b[39m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1515\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1516\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1517\u001b[39m lm_logits = \u001b[38;5;28mself\u001b[39m.lm_head(outputs[\u001b[32m0\u001b[39m]) + \u001b[38;5;28mself\u001b[39m.final_logits_bias\n\u001b[32m   1519\u001b[39m masked_lm_loss = \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/venv/lib/python3.13/site-packages/torch/nn/modules/module.py:1739\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1737\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1738\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1739\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/venv/lib/python3.13/site-packages/torch/nn/modules/module.py:1750\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1745\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1746\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1747\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1748\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1749\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1750\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1752\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1753\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/venv/lib/python3.13/site-packages/transformers/models/marian/modeling_marian.py:1252\u001b[39m, in \u001b[36mMarianModel.forward\u001b[39m\u001b[34m(self, input_ids, attention_mask, decoder_input_ids, decoder_attention_mask, head_mask, decoder_head_mask, cross_attn_head_mask, encoder_outputs, past_key_values, inputs_embeds, decoder_inputs_embeds, use_cache, output_attentions, output_hidden_states, return_dict, cache_position)\u001b[39m\n\u001b[32m   1245\u001b[39m     encoder_outputs = BaseModelOutput(\n\u001b[32m   1246\u001b[39m         last_hidden_state=encoder_outputs[\u001b[32m0\u001b[39m],\n\u001b[32m   1247\u001b[39m         hidden_states=encoder_outputs[\u001b[32m1\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(encoder_outputs) > \u001b[32m1\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m   1248\u001b[39m         attentions=encoder_outputs[\u001b[32m2\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(encoder_outputs) > \u001b[32m2\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m   1249\u001b[39m     )\n\u001b[32m   1251\u001b[39m \u001b[38;5;66;03m# decoder outputs consists of (dec_features, past_key_value, dec_hidden, dec_attn)\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1252\u001b[39m decoder_outputs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdecoder\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1253\u001b[39m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdecoder_input_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1254\u001b[39m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdecoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1255\u001b[39m \u001b[43m    \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m=\u001b[49m\u001b[43mencoder_outputs\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1256\u001b[39m \u001b[43m    \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1257\u001b[39m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdecoder_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1258\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcross_attn_head_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcross_attn_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1259\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1260\u001b[39m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdecoder_inputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1261\u001b[39m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[43m=\u001b[49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1262\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1263\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1264\u001b[39m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1265\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1266\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1268\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m return_dict:\n\u001b[32m   1269\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m decoder_outputs + encoder_outputs\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/venv/lib/python3.13/site-packages/torch/nn/modules/module.py:1739\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1737\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1738\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1739\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/venv/lib/python3.13/site-packages/torch/nn/modules/module.py:1750\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1745\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1746\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1747\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1748\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1749\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1750\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1752\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1753\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/venv/lib/python3.13/site-packages/transformers/models/marian/modeling_marian.py:1031\u001b[39m, in \u001b[36mMarianDecoder.forward\u001b[39m\u001b[34m(self, input_ids, attention_mask, encoder_hidden_states, encoder_attention_mask, head_mask, cross_attn_head_mask, past_key_values, inputs_embeds, use_cache, output_attentions, output_hidden_states, return_dict, cache_position)\u001b[39m\n\u001b[32m   1017\u001b[39m     layer_outputs = \u001b[38;5;28mself\u001b[39m._gradient_checkpointing_func(\n\u001b[32m   1018\u001b[39m         decoder_layer.\u001b[34m__call__\u001b[39m,\n\u001b[32m   1019\u001b[39m         hidden_states,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1028\u001b[39m         cache_position,\n\u001b[32m   1029\u001b[39m     )\n\u001b[32m   1030\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1031\u001b[39m     layer_outputs = \u001b[43mdecoder_layer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1032\u001b[39m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1033\u001b[39m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcausal_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1034\u001b[39m \u001b[43m        \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m=\u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1035\u001b[39m \u001b[43m        \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1036\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlayer_head_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1037\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcross_attn_layer_head_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1038\u001b[39m \u001b[43m            \u001b[49m\u001b[43mcross_attn_head_mask\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mcross_attn_head_mask\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\n\u001b[32m   1039\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1040\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1041\u001b[39m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1042\u001b[39m \u001b[43m        \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[43m=\u001b[49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1043\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1044\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1045\u001b[39m hidden_states = layer_outputs[\u001b[32m0\u001b[39m]\n\u001b[32m   1047\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m use_cache:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/venv/lib/python3.13/site-packages/torch/nn/modules/module.py:1739\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1737\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1738\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1739\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/venv/lib/python3.13/site-packages/torch/nn/modules/module.py:1750\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1745\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1746\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1747\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1748\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1749\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1750\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1752\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1753\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/venv/lib/python3.13/site-packages/transformers/models/marian/modeling_marian.py:447\u001b[39m, in \u001b[36mMarianDecoderLayer.forward\u001b[39m\u001b[34m(self, hidden_states, attention_mask, encoder_hidden_states, encoder_attention_mask, layer_head_mask, cross_attn_layer_head_mask, past_key_value, output_attentions, use_cache, cache_position)\u001b[39m\n\u001b[32m    445\u001b[39m hidden_states = \u001b[38;5;28mself\u001b[39m.activation_fn(\u001b[38;5;28mself\u001b[39m.fc1(hidden_states))\n\u001b[32m    446\u001b[39m hidden_states = nn.functional.dropout(hidden_states, p=\u001b[38;5;28mself\u001b[39m.activation_dropout, training=\u001b[38;5;28mself\u001b[39m.training)\n\u001b[32m--> \u001b[39m\u001b[32m447\u001b[39m hidden_states = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfc2\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    448\u001b[39m hidden_states = nn.functional.dropout(hidden_states, p=\u001b[38;5;28mself\u001b[39m.dropout, training=\u001b[38;5;28mself\u001b[39m.training)\n\u001b[32m    449\u001b[39m hidden_states = residual + hidden_states\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/venv/lib/python3.13/site-packages/torch/nn/modules/module.py:1739\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1737\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1738\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1739\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/venv/lib/python3.13/site-packages/torch/nn/modules/module.py:1750\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1745\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1746\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1747\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1748\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1749\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1750\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1752\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1753\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/venv/lib/python3.13/site-packages/torch/nn/modules/linear.py:125\u001b[39m, in \u001b[36mLinear.forward\u001b[39m\u001b[34m(self, input)\u001b[39m\n\u001b[32m    124\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) -> Tensor:\n\u001b[32m--> \u001b[39m\u001b[32m125\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# --- Blok 0: Inisialisasi, Fungsi Global, dan Pemuatan Data ---\n",
    "import pandas as pd\n",
    "import re\n",
    "import string\n",
    "import nltk\n",
    "import ssl\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords as nltk_stopwords, sentiwordnet as swn\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tag import pos_tag\n",
    "from afinn import Afinn\n",
    "from textblob import TextBlob\n",
    "from senticnet.senticnet import SenticNet # Pastikan senticnet terinstal\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import time\n",
    "import os # Untuk memeriksa keberadaan file\n",
    "\n",
    "# --- Konfigurasi dan Download Resource NLTK ---\n",
    "def download_nltk_resources():\n",
    "    try:\n",
    "        _create_unverified_https_context = ssl._create_unverified_context\n",
    "    except AttributeError:\n",
    "        pass\n",
    "    else:\n",
    "        ssl._create_default_https_context = _create_unverified_https_context\n",
    "\n",
    "    resources = {\n",
    "        \"punkt\": \"tokenizers/punkt\",\n",
    "        \"stopwords\": \"corpora/stopwords\",\n",
    "        \"averaged_perceptron_tagger\": \"taggers/averaged_perceptron_tagger\",\n",
    "        \"wordnet\": \"corpora/wordnet\",\n",
    "        \"omw-1.4\": \"corpora/omw-1.4\",\n",
    "        \"sentiwordnet\": \"corpora/sentiwordnet\"\n",
    "    }\n",
    "    \n",
    "    for resource_name, resource_path in resources.items():\n",
    "        try:\n",
    "            nltk.data.find(resource_path)\n",
    "            print(f\"[INFO] NLTK resource '{resource_name}' ({resource_path}) sudah ada.\")\n",
    "        except LookupError:\n",
    "            print(f\"[INFO] NLTK resource '{resource_name}' ({resource_path}) tidak ditemukan. Mengunduh...\")\n",
    "            try:\n",
    "                nltk.download(resource_name, quiet=True)\n",
    "                print(f\"[INFO] Berhasil mengunduh '{resource_name}'.\")\n",
    "            except Exception as e:\n",
    "                print(f\"[ERROR] Gagal mengunduh '{resource_name}': {e}\")\n",
    "                print(f\"         Coba jalankan secara manual: nltk.download('{resource_name}')\")\n",
    "        except Exception as e:\n",
    "            print(f\"[ERROR] Error saat memeriksa resource {resource_name}: {e}\")\n",
    "\n",
    "download_nltk_resources()\n",
    "print(\"-\" * 50)\n",
    "\n",
    "# Fungsi untuk memberi label sentimen berdasarkan skor\n",
    "def label_sentiment(score, pos_thresh=0.05, neg_thresh=-0.05):\n",
    "    if score > pos_thresh:\n",
    "        return \"Positif\"\n",
    "    elif score < neg_thresh:\n",
    "        return \"Negatif\"\n",
    "    else:\n",
    "        return \"Netral\"\n",
    "\n",
    "# --- Pemuatan Data dan Sampling ---\n",
    "csv_file_path_original = 'Data Pakai Label.csv'\n",
    "csv_file_path_sample_translated = 'Data_Sample_Translated.csv' # File untuk menyimpan/memuat sampel terjemahan\n",
    "translated_column_name = 'translated_content'\n",
    "sample_size = 100\n",
    "content_column_original = 'content' # Kolom teks asli Bahasa Indonesia\n",
    "\n",
    "df_sampled_translated = None\n",
    "\n",
    "# Coba muat sampel yang sudah ditranslasi jika ada\n",
    "if os.path.exists(csv_file_path_sample_translated):\n",
    "    try:\n",
    "        df_sampled_translated = pd.read_csv(csv_file_path_sample_translated)\n",
    "        if translated_column_name in df_sampled_translated.columns and len(df_sampled_translated) == sample_size:\n",
    "            print(f\"[INFO] Berhasil memuat sampel yang sudah ditranslasi dari: {csv_file_path_sample_translated}\")\n",
    "        else:\n",
    "            print(f\"[INFO] File sampel terjemahan ditemukan ({csv_file_path_sample_translated}) tetapi format/ukuran tidak sesuai. Akan dibuat ulang.\")\n",
    "            df_sampled_translated = None # Reset agar dibuat ulang\n",
    "    except Exception as e:\n",
    "        print(f\"[PERINGATAN] Gagal memuat file sampel terjemahan ({csv_file_path_sample_translated}): {e}. Akan dibuat ulang.\")\n",
    "        df_sampled_translated = None\n",
    "\n",
    "if df_sampled_translated is None:\n",
    "    try:\n",
    "        df_full = pd.read_csv(csv_file_path_original)\n",
    "        print(f\"[INFO] Berhasil memuat file asli: {csv_file_path_original}\")\n",
    "        print(f\"Jumlah baris awal (keseluruhan): {len(df_full)}\")\n",
    "\n",
    "        if content_column_original not in df_full.columns:\n",
    "            print(f\"[ERROR] Kolom '{content_column_original}' tidak ditemukan di {csv_file_path_original}.\")\n",
    "            exit()\n",
    "\n",
    "        # --- SAMPLING DATA ---\n",
    "        if len(df_full) > sample_size:\n",
    "            print(f\"[INFO] Melakukan sampling sebanyak {sample_size} baris dari dataset asli.\")\n",
    "            df_to_process = df_full.sample(n=sample_size, random_state=42).reset_index(drop=True)\n",
    "        else:\n",
    "            print(f\"[INFO] Ukuran dataset ({len(df_full)}) <= sample_size ({sample_size}). Menggunakan seluruh dataset.\")\n",
    "            df_to_process = df_full.copy()\n",
    "        \n",
    "        print(f\"Jumlah baris untuk diproses/translasi: {len(df_to_process)}\")\n",
    "\n",
    "        # --- FASE 0 (OPSIONAL): Translasi Teks dari Bahasa Indonesia ke Bahasa Inggris ---\n",
    "        # Jika Anda ingin melakukan translasi otomatis PADA SAMPEL, uncomment bagian di bawah.\n",
    "        # Pastikan Anda sudah `pip install transformers sentencepiece sacremoses`\n",
    "        \n",
    "        from transformers import MarianMTModel, MarianTokenizer\n",
    "        model_name_translation = \"Helsinki-NLP/opus-mt-id-en\"\n",
    "\n",
    "        def translate_batch_hf(texts_series, model_name=\"Helsinki-NLP/opus-mt-id-en\", batch_size=8):\n",
    "            print(f\"[INFO] Memuat model translasi: {model_name}\")\n",
    "            tokenizer = MarianTokenizer.from_pretrained(model_name)\n",
    "            model = MarianMTModel.from_pretrained(model_name)\n",
    "            print(\"[INFO] Model translasi dimuat.\")\n",
    "            \n",
    "            texts = texts_series.tolist()\n",
    "            translated_texts = []\n",
    "            num_batches = (len(texts) + batch_size - 1) // batch_size\n",
    "            \n",
    "            for i in range(num_batches):\n",
    "                start_idx = i * batch_size\n",
    "                end_idx = min((i + 1) * batch_size, len(texts))\n",
    "                batch_texts_cleaned = [str(text) if pd.notnull(text) else \"\" for text in texts[start_idx:end_idx]]\n",
    "                \n",
    "                print(f\"[INFO] Menerjemahkan batch {i+1}/{num_batches} (ukuran: {len(batch_texts_cleaned)})...\")\n",
    "                try:\n",
    "                    inputs = tokenizer(batch_texts_cleaned, return_tensors=\"pt\", padding=True, truncation=True, max_length=512)\n",
    "                    translated_tokens = model.generate(**inputs)\n",
    "                    batch_translated = [tokenizer.decode(t, skip_special_tokens=True) for t in translated_tokens]\n",
    "                    translated_texts.extend(batch_translated)\n",
    "                except Exception as e:\n",
    "                    print(f\"[ERROR] Gagal menerjemahkan batch {i+1}: {e}\")\n",
    "                    translated_texts.extend([\"\"] * len(batch_texts_cleaned))\n",
    "                time.sleep(0.1)\n",
    "            return pd.Series(translated_texts, index=texts_series.index)\n",
    "\n",
    "        # Lakukan translasi HANYA PADA SAMPEL\n",
    "        print(\"[INFO] Memulai proses translasi untuk data sampel...\")\n",
    "        start_translate_time = time.time()\n",
    "        df_to_process[translated_column_name] = translate_batch_hf(df_to_process[content_column_original])\n",
    "        end_translate_time = time.time()\n",
    "        print(f\"[INFO] Proses translasi sampel selesai dalam {end_translate_time - start_translate_time:.2f} detik.\")\n",
    "        \n",
    "        # *** BAGIAN INI YANG DITAMBAHKAN UNTUK KASUS TRANSLASI MANUAL ***\n",
    "        # Jika Anda melakukan translasi manual, pastikan kolom hasil translasi sudah ada di `df_full`\n",
    "        # dan skrip akan mengambilnya saat sampling.\n",
    "        # Jika Anda uncomment blok translasi di atas, baris ini tidak akan dieksekusi\n",
    "        # karena `translated_column_name` sudah dibuat.\n",
    "        if translated_column_name not in df_to_process.columns:\n",
    "            # Ini adalah placeholder jika translasi tidak dilakukan di skrip.\n",
    "            # Idealnya, Anda sudah punya kolom ini di CSV asli jika translasi manual.\n",
    "            print(f\"[PERINGATAN] Kolom '{translated_column_name}' tidak ada. Akan dibuat placeholder dari '{content_column_original}'.\")\n",
    "            print(\"           Analisis mungkin tidak akurat. Pastikan Anda sudah menerjemahkan data Anda ke kolom ini.\")\n",
    "            df_to_process[translated_column_name] = df_to_process[content_column_original]\n",
    "        # *** AKHIR BAGIAN TAMBAHAN ***\n",
    "\n",
    "        df_sampled_translated = df_to_process.copy()\n",
    "\n",
    "        # Simpan sampel yang sudah (atau dianggap sudah) ditranslasi\n",
    "        try:\n",
    "            df_sampled_translated.to_csv(csv_file_path_sample_translated, index=False, encoding='utf-8-sig')\n",
    "            print(f\"[INFO] Sampel yang (dianggap sudah) ditranslasi disimpan ke: {csv_file_path_sample_translated}\")\n",
    "        except Exception as e:\n",
    "            print(f\"[ERROR] Gagal menyimpan file sampel terjemahan: {e}\")\n",
    "\n",
    "    except FileNotFoundError:\n",
    "        print(f\"[ERROR] File asli '{csv_file_path_original}' tidak ditemukan.\")\n",
    "        exit()\n",
    "    except Exception as e:\n",
    "        print(f\"[ERROR] Terjadi kesalahan saat memuat atau sampling data: {e}\")\n",
    "        exit()\n",
    "\n",
    "if df_sampled_translated is None:\n",
    "    print(f\"[ERROR] Gagal memuat atau membuat data sampel terjemahan. Harap periksa error sebelumnya.\")\n",
    "    exit()\n",
    "\n",
    "df = df_sampled_translated.copy() # df sekarang adalah sampel yang sudah ada kolom terjemahannya\n",
    "print(f\"Jumlah baris data yang akan dianalisis: {len(df)}\")\n",
    "print(\"Contoh data yang akan dianalisis (2 baris pertama):\")\n",
    "print(df.head(2))\n",
    "print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "abd70960",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[FASE 1] Memulai preprocessing teks Bahasa Inggris...\n",
      "Jumlah baris setelah menghapus NaN di 'translated_content': 10798\n"
     ]
    },
    {
     "ename": "LookupError",
     "evalue": "\n**********************************************************************\n  Resource \u001b[93mpunkt_tab\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt_tab')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtokenizers/punkt_tab/english/\u001b[0m\n\n  Searched in:\n    - '/home/xerces/nltk_data'\n    - '/home/xerces/anaconda3/envs/venv/nltk_data'\n    - '/home/xerces/anaconda3/envs/venv/share/nltk_data'\n    - '/home/xerces/anaconda3/envs/venv/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n**********************************************************************\n",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mLookupError\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[13]\u001b[39m\u001b[32m, line 83\u001b[39m\n\u001b[32m     80\u001b[39m df[\u001b[33m'\u001b[39m\u001b[33mprocessed_text\u001b[39m\u001b[33m'\u001b[39m] = df[\u001b[33m'\u001b[39m\u001b[33mprocessed_text\u001b[39m\u001b[33m'\u001b[39m].apply(remove_numbers)\n\u001b[32m     81\u001b[39m df[\u001b[33m'\u001b[39m\u001b[33mprocessed_text\u001b[39m\u001b[33m'\u001b[39m] = df[\u001b[33m'\u001b[39m\u001b[33mprocessed_text\u001b[39m\u001b[33m'\u001b[39m].apply(remove_extra_whitespace)\n\u001b[32m---> \u001b[39m\u001b[32m83\u001b[39m df[\u001b[33m'\u001b[39m\u001b[33mtokens\u001b[39m\u001b[33m'\u001b[39m] = \u001b[43mdf\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mprocessed_text\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m.\u001b[49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mword_tokenize\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     84\u001b[39m \u001b[38;5;66;03m# STOPWORD REMOVAL DI-NONAKTIFKAN untuk dictionary based, bisa diaktifkan jika diperlukan\u001b[39;00m\n\u001b[32m     85\u001b[39m \u001b[38;5;66;03m# df['tokens_no_stopwords'] = df['tokens'].apply(remove_en_stopwords)\u001b[39;00m\n\u001b[32m     87\u001b[39m df[\u001b[33m'\u001b[39m\u001b[33mlemmatized_tokens\u001b[39m\u001b[33m'\u001b[39m] = df[\u001b[33m'\u001b[39m\u001b[33mtokens\u001b[39m\u001b[33m'\u001b[39m].apply(lemmatize_tokens_with_pos)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/venv/lib/python3.13/site-packages/pandas/core/series.py:4924\u001b[39m, in \u001b[36mSeries.apply\u001b[39m\u001b[34m(self, func, convert_dtype, args, by_row, **kwargs)\u001b[39m\n\u001b[32m   4789\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mapply\u001b[39m(\n\u001b[32m   4790\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   4791\u001b[39m     func: AggFuncType,\n\u001b[32m   (...)\u001b[39m\u001b[32m   4796\u001b[39m     **kwargs,\n\u001b[32m   4797\u001b[39m ) -> DataFrame | Series:\n\u001b[32m   4798\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   4799\u001b[39m \u001b[33;03m    Invoke function on values of Series.\u001b[39;00m\n\u001b[32m   4800\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m   4915\u001b[39m \u001b[33;03m    dtype: float64\u001b[39;00m\n\u001b[32m   4916\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m   4917\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mSeriesApply\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   4918\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   4919\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4920\u001b[39m \u001b[43m        \u001b[49m\u001b[43mconvert_dtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconvert_dtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4921\u001b[39m \u001b[43m        \u001b[49m\u001b[43mby_row\u001b[49m\u001b[43m=\u001b[49m\u001b[43mby_row\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4922\u001b[39m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[43m=\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4923\u001b[39m \u001b[43m        \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m-> \u001b[39m\u001b[32m4924\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/venv/lib/python3.13/site-packages/pandas/core/apply.py:1427\u001b[39m, in \u001b[36mSeriesApply.apply\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1424\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.apply_compat()\n\u001b[32m   1426\u001b[39m \u001b[38;5;66;03m# self.func is Callable\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1427\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mapply_standard\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/venv/lib/python3.13/site-packages/pandas/core/apply.py:1507\u001b[39m, in \u001b[36mSeriesApply.apply_standard\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1501\u001b[39m \u001b[38;5;66;03m# row-wise access\u001b[39;00m\n\u001b[32m   1502\u001b[39m \u001b[38;5;66;03m# apply doesn't have a `na_action` keyword and for backward compat reasons\u001b[39;00m\n\u001b[32m   1503\u001b[39m \u001b[38;5;66;03m# we need to give `na_action=\"ignore\"` for categorical data.\u001b[39;00m\n\u001b[32m   1504\u001b[39m \u001b[38;5;66;03m# TODO: remove the `na_action=\"ignore\"` when that default has been changed in\u001b[39;00m\n\u001b[32m   1505\u001b[39m \u001b[38;5;66;03m#  Categorical (GH51645).\u001b[39;00m\n\u001b[32m   1506\u001b[39m action = \u001b[33m\"\u001b[39m\u001b[33mignore\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(obj.dtype, CategoricalDtype) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1507\u001b[39m mapped = \u001b[43mobj\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_map_values\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1508\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmapper\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcurried\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mna_action\u001b[49m\u001b[43m=\u001b[49m\u001b[43maction\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconvert\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mconvert_dtype\u001b[49m\n\u001b[32m   1509\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1511\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(mapped) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(mapped[\u001b[32m0\u001b[39m], ABCSeries):\n\u001b[32m   1512\u001b[39m     \u001b[38;5;66;03m# GH#43986 Need to do list(mapped) in order to get treated as nested\u001b[39;00m\n\u001b[32m   1513\u001b[39m     \u001b[38;5;66;03m#  See also GH#25959 regarding EA support\u001b[39;00m\n\u001b[32m   1514\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m obj._constructor_expanddim(\u001b[38;5;28mlist\u001b[39m(mapped), index=obj.index)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/venv/lib/python3.13/site-packages/pandas/core/base.py:921\u001b[39m, in \u001b[36mIndexOpsMixin._map_values\u001b[39m\u001b[34m(self, mapper, na_action, convert)\u001b[39m\n\u001b[32m    918\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(arr, ExtensionArray):\n\u001b[32m    919\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m arr.map(mapper, na_action=na_action)\n\u001b[32m--> \u001b[39m\u001b[32m921\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43malgorithms\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmap_array\u001b[49m\u001b[43m(\u001b[49m\u001b[43marr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmapper\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mna_action\u001b[49m\u001b[43m=\u001b[49m\u001b[43mna_action\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconvert\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/venv/lib/python3.13/site-packages/pandas/core/algorithms.py:1743\u001b[39m, in \u001b[36mmap_array\u001b[39m\u001b[34m(arr, mapper, na_action, convert)\u001b[39m\n\u001b[32m   1741\u001b[39m values = arr.astype(\u001b[38;5;28mobject\u001b[39m, copy=\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[32m   1742\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m na_action \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1743\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mlib\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmap_infer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmapper\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconvert\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1744\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1745\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m lib.map_infer_mask(\n\u001b[32m   1746\u001b[39m         values, mapper, mask=isna(values).view(np.uint8), convert=convert\n\u001b[32m   1747\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mlib.pyx:2972\u001b[39m, in \u001b[36mpandas._libs.lib.map_infer\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/venv/lib/python3.13/site-packages/nltk/tokenize/__init__.py:142\u001b[39m, in \u001b[36mword_tokenize\u001b[39m\u001b[34m(text, language, preserve_line)\u001b[39m\n\u001b[32m    127\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mword_tokenize\u001b[39m(text, language=\u001b[33m\"\u001b[39m\u001b[33menglish\u001b[39m\u001b[33m\"\u001b[39m, preserve_line=\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[32m    128\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    129\u001b[39m \u001b[33;03m    Return a tokenized copy of *text*,\u001b[39;00m\n\u001b[32m    130\u001b[39m \u001b[33;03m    using NLTK's recommended word tokenizer\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    140\u001b[39m \u001b[33;03m    :type preserve_line: bool\u001b[39;00m\n\u001b[32m    141\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m142\u001b[39m     sentences = [text] \u001b[38;5;28;01mif\u001b[39;00m preserve_line \u001b[38;5;28;01melse\u001b[39;00m \u001b[43msent_tokenize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlanguage\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    143\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m [\n\u001b[32m    144\u001b[39m         token \u001b[38;5;28;01mfor\u001b[39;00m sent \u001b[38;5;129;01min\u001b[39;00m sentences \u001b[38;5;28;01mfor\u001b[39;00m token \u001b[38;5;129;01min\u001b[39;00m _treebank_word_tokenizer.tokenize(sent)\n\u001b[32m    145\u001b[39m     ]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/venv/lib/python3.13/site-packages/nltk/tokenize/__init__.py:119\u001b[39m, in \u001b[36msent_tokenize\u001b[39m\u001b[34m(text, language)\u001b[39m\n\u001b[32m    109\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34msent_tokenize\u001b[39m(text, language=\u001b[33m\"\u001b[39m\u001b[33menglish\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m    110\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    111\u001b[39m \u001b[33;03m    Return a sentence-tokenized copy of *text*,\u001b[39;00m\n\u001b[32m    112\u001b[39m \u001b[33;03m    using NLTK's recommended sentence tokenizer\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    117\u001b[39m \u001b[33;03m    :param language: the model name in the Punkt corpus\u001b[39;00m\n\u001b[32m    118\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m119\u001b[39m     tokenizer = \u001b[43m_get_punkt_tokenizer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlanguage\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    120\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m tokenizer.tokenize(text)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/venv/lib/python3.13/site-packages/nltk/tokenize/__init__.py:105\u001b[39m, in \u001b[36m_get_punkt_tokenizer\u001b[39m\u001b[34m(language)\u001b[39m\n\u001b[32m     96\u001b[39m \u001b[38;5;129m@functools\u001b[39m.lru_cache\n\u001b[32m     97\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_get_punkt_tokenizer\u001b[39m(language=\u001b[33m\"\u001b[39m\u001b[33menglish\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m     98\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m     99\u001b[39m \u001b[33;03m    A constructor for the PunktTokenizer that utilizes\u001b[39;00m\n\u001b[32m    100\u001b[39m \u001b[33;03m    a lru cache for performance.\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    103\u001b[39m \u001b[33;03m    :type language: str\u001b[39;00m\n\u001b[32m    104\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m105\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mPunktTokenizer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlanguage\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/venv/lib/python3.13/site-packages/nltk/tokenize/punkt.py:1744\u001b[39m, in \u001b[36mPunktTokenizer.__init__\u001b[39m\u001b[34m(self, lang)\u001b[39m\n\u001b[32m   1742\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, lang=\u001b[33m\"\u001b[39m\u001b[33menglish\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m   1743\u001b[39m     PunktSentenceTokenizer.\u001b[34m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m)\n\u001b[32m-> \u001b[39m\u001b[32m1744\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mload_lang\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlang\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/venv/lib/python3.13/site-packages/nltk/tokenize/punkt.py:1749\u001b[39m, in \u001b[36mPunktTokenizer.load_lang\u001b[39m\u001b[34m(self, lang)\u001b[39m\n\u001b[32m   1746\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mload_lang\u001b[39m(\u001b[38;5;28mself\u001b[39m, lang=\u001b[33m\"\u001b[39m\u001b[33menglish\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m   1747\u001b[39m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnltk\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdata\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m find\n\u001b[32m-> \u001b[39m\u001b[32m1749\u001b[39m     lang_dir = \u001b[43mfind\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43mf\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtokenizers/punkt_tab/\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mlang\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[33;43m/\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m   1750\u001b[39m     \u001b[38;5;28mself\u001b[39m._params = load_punkt_params(lang_dir)\n\u001b[32m   1751\u001b[39m     \u001b[38;5;28mself\u001b[39m._lang = lang\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/venv/lib/python3.13/site-packages/nltk/data.py:579\u001b[39m, in \u001b[36mfind\u001b[39m\u001b[34m(resource_name, paths)\u001b[39m\n\u001b[32m    577\u001b[39m sep = \u001b[33m\"\u001b[39m\u001b[33m*\u001b[39m\u001b[33m\"\u001b[39m * \u001b[32m70\u001b[39m\n\u001b[32m    578\u001b[39m resource_not_found = \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mmsg\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m--> \u001b[39m\u001b[32m579\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m(resource_not_found)\n",
      "\u001b[31mLookupError\u001b[39m: \n**********************************************************************\n  Resource \u001b[93mpunkt_tab\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt_tab')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtokenizers/punkt_tab/english/\u001b[0m\n\n  Searched in:\n    - '/home/xerces/nltk_data'\n    - '/home/xerces/anaconda3/envs/venv/nltk_data'\n    - '/home/xerces/anaconda3/envs/venv/share/nltk_data'\n    - '/home/xerces/anaconda3/envs/venv/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n**********************************************************************\n"
     ]
    }
   ],
   "source": [
    "# --- Blok 1: Preprocessing Teks Bahasa Inggris ---\n",
    "print(\"[FASE 1] Memulai preprocessing teks Bahasa Inggris...\")\n",
    "start_fase1_time = time.time()\n",
    "\n",
    "# Fungsi-fungsi preprocessing\n",
    "def to_lowercase(text):\n",
    "    return text.lower() if isinstance(text, str) else \"\"\n",
    "\n",
    "def remove_urls(text):\n",
    "    return re.sub(r'http\\S+|www\\S+|https\\S+', '', text, flags=re.MULTILINE)\n",
    "\n",
    "def remove_mentions_hashtags(text):\n",
    "    text = re.sub(r'@\\w+', '', text)\n",
    "    text = re.sub(r'#\\w+', '', text)\n",
    "    return text\n",
    "\n",
    "def remove_punctuation(text):\n",
    "    # Simpan tanda apostrof karena penting untuk kontraksi (misal, \"don't\")\n",
    "    # punctuation_to_remove = string.punctuation.replace(\"'\", \"\") \n",
    "    # return text.translate(str.maketrans('', '', punctuation_to_remove))\n",
    "    # Untuk dictionary-based, seringkali lebih baik menghilangkan semua punctuation\n",
    "    return text.translate(str.maketrans('', '', string.punctuation))\n",
    "\n",
    "\n",
    "def remove_numbers(text):\n",
    "    return re.sub(r'\\d+', '', text)\n",
    "\n",
    "def remove_extra_whitespace(text):\n",
    "    return \" \".join(text.split())\n",
    "\n",
    "stop_words_en = set(nltk_stopwords.words('english'))\n",
    "# Kata-kata negasi penting, jangan dihapus jika ada di kamus\n",
    "negation_words = {\"not\", \"no\", \"n't\", \"never\", \"don't\", \"can't\", \"isn't\"}\n",
    "stop_words_en = stop_words_en - negation_words\n",
    "\n",
    "def remove_en_stopwords(tokens):\n",
    "    return [word for word in tokens if word not in stop_words_en]\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "def nltk_tag_to_wordnet_tag(nltk_tag):\n",
    "    if nltk_tag.startswith('J'):\n",
    "        return nltk.corpus.wordnet.ADJ\n",
    "    elif nltk_tag.startswith('V'):\n",
    "        return nltk.corpus.wordnet.VERB\n",
    "    elif nltk_tag.startswith('N'):\n",
    "        return nltk.corpus.wordnet.NOUN\n",
    "    elif nltk_tag.startswith('R'):\n",
    "        return nltk.corpus.wordnet.ADV\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "def lemmatize_tokens_with_pos(tokens):\n",
    "    nltk_tagged = nltk.pos_tag(tokens)\n",
    "    wordnet_tagged = map(lambda x: (x[0], nltk_tag_to_wordnet_tag(x[1])), nltk_tagged)\n",
    "    lemmatized_sentence = []\n",
    "    for word, tag in wordnet_tagged:\n",
    "        if tag is None:\n",
    "            lemmatized_sentence.append(word)\n",
    "        else:\n",
    "            lemmatized_sentence.append(lemmatizer.lemmatize(word, tag))\n",
    "    return lemmatized_sentence\n",
    "\n",
    "# Pengecekan kolom hasil terjemahan\n",
    "if translated_column_name not in df.columns:\n",
    "    print(f\"[ERROR] Kolom terjemahan '{translated_column_name}' tidak ada di DataFrame. Pastikan Fase 0 (Translasi) sudah benar.\")\n",
    "    exit()\n",
    "\n",
    "# Menghapus baris jika teks terjemahan kosong\n",
    "df.dropna(subset=[translated_column_name], inplace=True)\n",
    "df.reset_index(drop=True, inplace=True)\n",
    "print(f\"Jumlah baris setelah menghapus NaN di '{translated_column_name}': {len(df)}\")\n",
    "\n",
    "# Pipeline Preprocessing\n",
    "df['processed_text'] = df[translated_column_name].apply(to_lowercase)\n",
    "df['processed_text'] = df['processed_text'].apply(remove_urls)\n",
    "df['processed_text'] = df['processed_text'].apply(remove_mentions_hashtags)\n",
    "df['textblob_ready_text'] = df['processed_text'].copy() # Untuk TextBlob, sebelum hapus tanda baca sepenuhnya\n",
    "\n",
    "df['processed_text'] = df['processed_text'].apply(remove_punctuation)\n",
    "df['processed_text'] = df['processed_text'].apply(remove_numbers)\n",
    "df['processed_text'] = df['processed_text'].apply(remove_extra_whitespace)\n",
    "\n",
    "df['tokens'] = df['processed_text'].apply(word_tokenize)\n",
    "# STOPWORD REMOVAL DI-NONAKTIFKAN untuk dictionary based, bisa diaktifkan jika diperlukan\n",
    "# df['tokens_no_stopwords'] = df['tokens'].apply(remove_en_stopwords)\n",
    "\n",
    "df['lemmatized_tokens'] = df['tokens'].apply(lemmatize_tokens_with_pos)\n",
    "\n",
    "end_fase1_time = time.time()\n",
    "print(f\"[FASE 1] Preprocessing teks selesai dalam {end_fase1_time - start_fase1_time:.2f} detik.\")\n",
    "print(\"\\nContoh hasil preprocessing (5 baris pertama):\")\n",
    "print(df[[translated_column_name, 'processed_text', 'tokens', 'lemmatized_tokens']].head())\n",
    "print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "075e5538",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Blok 2.1: Analisis Sentimen dengan AFINN ---\n",
    "print(\"[FASE 2.1] Menerapkan analisis sentimen AFINN...\")\n",
    "start_fase2_1_time = time.time()\n",
    "\n",
    "afinn_lexicon = Afinn()\n",
    "def get_afinn_score(tokens):\n",
    "    if not tokens: return 0\n",
    "    # Afinn library bisa langsung menerima string, join token lebih baik\n",
    "    return afinn_lexicon.score(\" \".join(tokens))\n",
    "\n",
    "df['afinn_score'] = df['tokens'].apply(get_afinn_score) # Menggunakan token asli (bukan lemmatized)\n",
    "df['afinn_label'] = df['afinn_score'].apply(lambda x: label_sentiment(x, pos_thresh=0.1, neg_thresh=-0.1))\n",
    "\n",
    "end_fase2_1_time = time.time()\n",
    "print(f\"[FASE 2.1] Analisis AFINN selesai dalam {end_fase2_1_time - start_fase2_1_time:.2f} detik.\")\n",
    "print(\"\\nContoh hasil AFINN:\")\n",
    "print(df[[translated_column_name, 'afinn_score', 'afinn_label']].head())\n",
    "print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cc220a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Blok 2.2: Analisis Sentimen dengan SentiWordNet ---\n",
    "print(\"[FASE 2.2] Menerapkan analisis sentimen SentiWordNet...\")\n",
    "start_fase2_2_time = time.time()\n",
    "\n",
    "def get_sentiwordnet_score(lemmatized_tokens_with_pos_tags):\n",
    "    if not lemmatized_tokens_with_pos_tags:\n",
    "        return 0.0, 0.0, 0.0\n",
    "\n",
    "    sentiment_score_doc = 0.0\n",
    "    pos_score_doc = 0.0\n",
    "    neg_score_doc = 0.0\n",
    "    \n",
    "    # Kita sudah punya lemmatized_tokens, sekarang tinggal POS taggingnya\n",
    "    # Jika lemmatized_tokens sudah punya POS tag, langsung gunakan.\n",
    "    # Jika tidak, lakukan POS tagging pada lemmatized_tokens.\n",
    "    # Untuk SentiWordNet, kata harus dilematisasi DENGAN POS tag yang benar.\n",
    "\n",
    "    tagged_tokens = nltk.pos_tag(lemmatized_tokens_with_pos_tags) # POS tag pada token lemmatized\n",
    "\n",
    "    for word, tag in tagged_tokens:\n",
    "        wn_tag = nltk_tag_to_wordnet_tag(tag) # Fungsi ini sudah didefinisikan di Blok 1\n",
    "        if wn_tag not in (nltk.corpus.wordnet.NOUN, nltk.corpus.wordnet.ADJ, \n",
    "                           nltk.corpus.wordnet.ADV, nltk.corpus.wordnet.VERB):\n",
    "            continue\n",
    "\n",
    "        # Lemmatize lagi dengan POS tag yang benar untuk SentiWordNet\n",
    "        # (Meskipun sudah dilematisasi, terkadang bentuknya belum 100% dasar tanpa POS spesifik)\n",
    "        lemma = lemmatizer.lemmatize(word, wn_tag) if wn_tag else word \n",
    "        \n",
    "        synsets = list(swn.senti_synsets(lemma, wn_tag))\n",
    "        if not synsets:\n",
    "            # Jika tidak ada synset untuk lemma+tag, coba tanpa tag\n",
    "            synsets = list(swn.senti_synsets(lemma))\n",
    "            if not synsets:\n",
    "                continue\n",
    "        \n",
    "        # Ambil skor dari synset pertama (paling umum)\n",
    "        synset = synsets[0]\n",
    "        pos_score = synset.pos_score()\n",
    "        neg_score = synset.neg_score()\n",
    "        \n",
    "        pos_score_doc += pos_score\n",
    "        neg_score_doc += neg_score\n",
    "        sentiment_score_doc += (pos_score - neg_score)\n",
    "        \n",
    "    return pos_score_doc, neg_score_doc, sentiment_score_doc\n",
    "\n",
    "swn_scores = df['lemmatized_tokens'].apply(get_sentiwordnet_score)\n",
    "df['swn_pos_score'] = swn_scores.apply(lambda x: x[0])\n",
    "df['swn_neg_score'] = swn_scores.apply(lambda x: x[1])\n",
    "df['swn_sentiment_score'] = swn_scores.apply(lambda x: x[2])\n",
    "df['swn_label'] = df['swn_sentiment_score'].apply(label_sentiment)\n",
    "\n",
    "end_fase2_2_time = time.time()\n",
    "print(f\"[FASE 2.2] Analisis SentiWordNet selesai dalam {end_fase2_2_time - start_fase2_2_time:.2f} detik.\")\n",
    "print(\"\\nContoh hasil SentiWordNet:\")\n",
    "print(df[[translated_column_name, 'swn_sentiment_score', 'swn_label']].head())\n",
    "print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84c71132",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Blok 2.3: Analisis Sentimen dengan SenticNet ---\n",
    "print(\"[FASE 2.3] Menerapkan analisis sentimen SenticNet...\")\n",
    "start_fase2_3_time = time.time()\n",
    "\n",
    "sn = SenticNet()\n",
    "def get_senticnet_score(tokens_or_lemmas): # Bisa pakai tokens atau lemmatized_tokens\n",
    "    if not tokens_or_lemmas: return 0.0\n",
    "    \n",
    "    score = 0.0\n",
    "    count = 0\n",
    "    for token in tokens_or_lemmas:\n",
    "        try:\n",
    "            # SenticNet bekerja dengan konsep, jadi lowercase\n",
    "            concept_info = sn.concept(token.lower()) \n",
    "            polarity_value = float(concept_info['polarity_value'])\n",
    "            score += polarity_value\n",
    "            count += 1\n",
    "        except KeyError: # Konsep tidak ditemukan di SenticNet\n",
    "            continue\n",
    "    return score / count if count > 0 else 0.0\n",
    "\n",
    "# Coba dengan lemmatized_tokens karena SenticNet berbasis konsep\n",
    "df['senticnet_score'] = df['lemmatized_tokens'].apply(get_senticnet_score) \n",
    "df['senticnet_label'] = df['senticnet_score'].apply(label_sentiment)\n",
    "\n",
    "end_fase2_3_time = time.time()\n",
    "print(f\"[FASE 2.3] Analisis SenticNet selesai dalam {end_fase2_3_time - start_fase2_3_time:.2f} detik.\")\n",
    "print(\"\\nContoh hasil SenticNet:\")\n",
    "print(df[[translated_column_name, 'senticnet_score', 'senticnet_label']].head())\n",
    "print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9106c66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Blok 2.4: Analisis Sentimen dengan TextBlob ---\n",
    "print(\"[FASE 2.4] Menerapkan analisis sentimen TextBlob...\")\n",
    "start_fase2_4_time = time.time()\n",
    "\n",
    "def get_textblob_sentiment(text):\n",
    "    if not isinstance(text, str) or not text.strip(): # Handle empty or non-string\n",
    "        return 0.0, 0.0\n",
    "    blob = TextBlob(text)\n",
    "    return blob.sentiment.polarity, blob.sentiment.subjectivity\n",
    "\n",
    "# Gunakan 'textblob_ready_text' yang masih memiliki beberapa tanda baca\n",
    "textblob_sentiments = df['textblob_ready_text'].apply(get_textblob_sentiment)\n",
    "df['textblob_polarity'] = textblob_sentiments.apply(lambda x: x[0])\n",
    "df['textblob_subjectivity'] = textblob_sentiments.apply(lambda x: x[1])\n",
    "df['textblob_label'] = df['textblob_polarity'].apply(label_sentiment)\n",
    "\n",
    "end_fase2_4_time = time.time()\n",
    "print(f\"[FASE 2.4] Analisis TextBlob selesai dalam {end_fase2_4_time - start_fase2_4_time:.2f} detik.\")\n",
    "print(\"\\nContoh hasil TextBlob:\")\n",
    "print(df[[translated_column_name, 'textblob_polarity', 'textblob_label']].head())\n",
    "print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2188937",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Blok 3: Tampilkan Hasil Gabungan dari Semua Kamus ---\n",
    "print(\"[FASE 3] Menampilkan contoh hasil analisis gabungan (10 baris pertama):\")\n",
    "\n",
    "# Pastikan kolom Sentimen Akhir ada untuk perbandingan\n",
    "ground_truth_col = 'Sentimen Akhir'\n",
    "columns_to_display = ['content', translated_column_name]\n",
    "if ground_truth_col in df.columns:\n",
    "    columns_to_display.append(ground_truth_col)\n",
    "\n",
    "columns_to_display.extend([\n",
    "    'afinn_score', 'afinn_label', \n",
    "    'swn_sentiment_score', 'swn_label',\n",
    "    'senticnet_score', 'senticnet_label',\n",
    "    'textblob_polarity', 'textblob_label'\n",
    "])\n",
    "\n",
    "# Filter kolom yang benar-benar ada di DataFrame untuk menghindari error\n",
    "columns_to_display = [col for col in columns_to_display if col in df.columns]\n",
    "\n",
    "print(df[columns_to_display].head(10))\n",
    "print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a98d6f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Blok 4: Evaluasi dan Visualisasi ---\n",
    "print(\"[FASE 4] Melakukan evaluasi dan visualisasi...\")\n",
    "start_fase4_time = time.time()\n",
    "\n",
    "ground_truth_column = 'Sentimen Akhir' # Kolom sentimen manual Anda\n",
    "\n",
    "def evaluate_sentiments_detailed(y_true, y_pred, method_name, labels_order=[\"Positif\", \"Netral\", \"Negatif\"]):\n",
    "    # Pastikan y_true dan y_pred memiliki panjang yang sama dan tidak kosong\n",
    "    if len(y_true) == 0 or len(y_pred) == 0 or len(y_true) != len(y_pred):\n",
    "        print(f\"[PERINGATAN] Data tidak valid untuk evaluasi {method_name}. Panjang y_true: {len(y_true)}, y_pred: {len(y_pred)}\")\n",
    "        return None\n",
    "\n",
    "    # Hapus baris NaN dari y_true dan y_pred secara bersamaan\n",
    "    valid_indices = y_true.notna() & y_pred.notna()\n",
    "    y_true_cleaned = y_true[valid_indices]\n",
    "    y_pred_cleaned = y_pred[valid_indices]\n",
    "\n",
    "    if len(y_true_cleaned) == 0:\n",
    "        print(f\"[PERINGATAN] Tidak ada data valid setelah membersihkan NaN untuk evaluasi {method_name}.\")\n",
    "        return None\n",
    "\n",
    "    accuracy = accuracy_score(y_true_cleaned, y_pred_cleaned)\n",
    "    # Dapatkan semua label unik yang ada di y_true_cleaned dan y_pred_cleaned\n",
    "    unique_labels = sorted(list(set(y_true_cleaned.unique()) | set(y_pred_cleaned.unique())))\n",
    "    \n",
    "    # Filter labels_order agar hanya berisi label yang ada di data\n",
    "    current_labels_order = [l for l in labels_order if l in unique_labels]\n",
    "    if not current_labels_order: # Jika labels_order tidak relevan, gunakan unique_labels\n",
    "        current_labels_order = unique_labels\n",
    "\n",
    "    report = classification_report(y_true_cleaned, y_pred_cleaned, labels=current_labels_order, zero_division=0)\n",
    "    cm = confusion_matrix(y_true_cleaned, y_pred_cleaned, labels=current_labels_order)\n",
    "\n",
    "    print(f\"\\n--- Evaluasi untuk Metode: {method_name} ---\")\n",
    "    print(f\"Akurasi: {accuracy:.4f}\")\n",
    "    print(\"Laporan Klasifikasi:\")\n",
    "    print(report)\n",
    "    \n",
    "    plt.figure(figsize=(6,4))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "                xticklabels=current_labels_order, \n",
    "                yticklabels=current_labels_order)\n",
    "    plt.title(f'Confusion Matrix - {method_name}')\n",
    "    plt.xlabel('Predicted Label')\n",
    "    plt.ylabel('True Label')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return accuracy\n",
    "\n",
    "if ground_truth_column in df.columns:\n",
    "    print(f\"\\n[INFO] Melakukan evaluasi berdasarkan kolom '{ground_truth_column}'...\")\n",
    "    \n",
    "    # Pastikan tidak ada NaN di kolom ground_truth_column sebelum evaluasi\n",
    "    df_eval = df.copy() # Bekerja dengan salinan untuk evaluasi\n",
    "\n",
    "    print(f\"Jumlah baris sebelum menghapus NaN di '{ground_truth_column}': {len(df_eval)}\")\n",
    "    df_eval.dropna(subset=[ground_truth_column], inplace=True)\n",
    "    print(f\"Jumlah baris setelah menghapus NaN di '{ground_truth_column}': {len(df_eval)}\")\n",
    "    df_eval.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    if not df_eval.empty:\n",
    "        evaluate_sentiments_detailed(df_eval[ground_truth_column], df_eval['afinn_label'], \"AFINN\")\n",
    "        evaluate_sentiments_detailed(df_eval[ground_truth_column], df_eval['swn_label'], \"SentiWordNet\")\n",
    "        evaluate_sentiments_detailed(df_eval[ground_truth_column], df_eval['senticnet_label'], \"SenticNet\")\n",
    "        evaluate_sentiments_detailed(df_eval[ground_truth_column], df_eval['textblob_label'], \"TextBlob\")\n",
    "    else:\n",
    "        print(f\"[PERINGATAN] Tidak ada data ground truth yang valid di kolom '{ground_truth_column}' untuk evaluasi setelah pembersihan NaN.\")\n",
    "else:\n",
    "    print(f\"[INFO] Kolom ground truth '{ground_truth_column}' tidak ditemukan. Evaluasi dilewati.\")\n",
    "\n",
    "end_fase4_time = time.time()\n",
    "print(f\"[FASE 4] Evaluasi dan visualisasi selesai dalam {end_fase4_time - start_fase4_time:.2f} detik.\")\n",
    "print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edaa9886",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Blok 5: Penyimpanan Hasil Akhir ---\n",
    "print(\"[FASE 5] Menyimpan hasil akhir...\")\n",
    "\n",
    "output_csv_path = 'Data_Hasil_Sentiment_Dictionaries_Lengkap.csv'\n",
    "try:\n",
    "    df.to_csv(output_csv_path, index=False, encoding='utf-8-sig') # utf-8-sig untuk kompatibilitas Excel\n",
    "    print(f\"\\n[INFO] Hasil lengkap disimpan ke: {output_csv_path}\")\n",
    "except Exception as e:\n",
    "    print(f\"[ERROR] Gagal menyimpan file CSV: {e}\")\n",
    "\n",
    "# Waktu total jika semua blok dijalankan berurutan dari awal\n",
    "# total_execution_time = (end_fase1_time - start_fase1_time) + \\\n",
    "#                        (end_fase2_1_time - start_fase2_1_time) + \\\n",
    "#                        (end_fase2_2_time - start_fase2_2_time) + \\\n",
    "#                        (end_fase2_3_time - start_fase2_3_time) + \\\n",
    "#                        (end_fase2_4_time - start_fase2_4_time) + \\\n",
    "#                        (end_fase4_time - start_fase4_time) # Tambahkan waktu fase lain jika ada\n",
    "# print(f\"\\n[INFO] Perkiraan total waktu eksekusi untuk Fase 1-4 (tanpa I/O data awal & translasi): {total_execution_time:.2f} detik.\")\n",
    "print(\"--- Proses Selesai ---\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
